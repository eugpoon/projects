{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836f17a",
   "metadata": {},
   "source": [
    "# [WiDS Datathon 2023](https://www.kaggle.com/competitions/widsdatathon2023/overview)\n",
    "\n",
    "### Dependent Variable\n",
    "<details>\n",
    "    <summary> (click to expand)</summary>\n",
    "\n",
    "- **contest-tmp2m-14d__tmp2m**: the arithmetic mean of the max and min observed temperature over the next 14 days for each location and start date, computed as (measured max temperature + measured mini temperature) / 2\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Independent Variables\n",
    "<details>\n",
    "    <summary> (click to expand)</summary>\n",
    "    \n",
    "- **contest-slp-14d**: file containing sea level pressure (slp)\n",
    "- **nmme0-tmp2m-34w**: file containing most recent monthly NMME model forecasts for tmp2m (**cancm30, cancm40, ccsm30, ccsm40, cfsv20, gfdlflora0, gfdlflorb0, gfdl0, nasa0, nmme0mean**) and average forecast across those models (nmme0mean)\n",
    "- **contest-pres-sfc-gauss-14d**: pressure\n",
    "- **mjo1d**: MJO phase and amplitude\n",
    "- **contest-pevpr-sfc-gauss-14d**: potential evaporation\n",
    "- **contest-wind-h850-14d**: geopotential height at 850 millibars\n",
    "- **contest-wind-h500-14d**: geopotential height at 500 millibars\n",
    "- **contest-wind-h100-14d**: geopotential height at 100 millibars\n",
    "- **contest-wind-h10-14d**: geopotential height at 10 millibars\n",
    "- **contest-wind-vwnd-925-14d**: longitudinal wind at 925 millibars\n",
    "- **contest-wind-vwnd-250-14d**: longitudinal wind at 250 millibars\n",
    "- **contest-wind-uwnd-250-14d**: zonal wind at 250 millibars\n",
    "- **contest-wind-uwnd-925-14d**: zonal wind at 925 millibars\n",
    "- **contest-rhum-sig995-14d**: relative humidity\n",
    "- **contest-prwtr-eatm-14d**: precipitable water for entire atmosphere\n",
    "- **nmme-prate-34w**: weeks 3-4 weighted average of monthly NMME model forecasts for precipitation\n",
    "- **nmme-prate-56w**: weeks 5-6 weighted average of monthly NMME model forecasts for precipitation\n",
    "- **nmme0-prate-56w**: weeks 5-6 weighted average of most recent monthly NMME model forecasts for precipitation\n",
    "- **nmme0-prate-34w**: weeks 3-4 weighted average of most recent monthly NMME model forecasts for precipitation\n",
    "- **nmme-tmp2m-34w**: weeks 3-4 weighted average of most recent monthly NMME model forecasts for target label, contest-tmp2m-14d__tmp2m\n",
    "- **nmme-tmp2m-56w**: weeks 5-6 weighted average of monthly NMME model forecasts for target label, contest-tmp2m-14d__tmp2m\n",
    "- **mei**: MEI (mei), MEI rank (rank), and Niño Index Phase (nip)\n",
    "- **elevation**: elevation\n",
    "- **contest-precip-14d**: measured precipitation\n",
    "- **climateregions**: Köppen-Geigerclimateclassifications, string\n",
    "\n",
    "\n",
    "- **lat**: latitude of location (anonymized)\n",
    "- **lon**: longitude of location (anonymized)\n",
    "- **startdate**: startdate of the 14 day period\n",
    "- **sst**: sea surface temperature\n",
    "- **icec**: sea ice concentration\n",
    "- **cancm30, cancm40, ccsm30, ccsm40, cfsv20, gfdlflora0, gfdlflorb0, gfdl0, nasa0, nmme0mean**: most recent forecasts from weather models\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2b58b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.tabular.core import df_shrink\n",
    "import torch\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 100\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd21770",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74cea44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 cols with null: ['nmme0-tmp2m-34w__ccsm30', 'nmme-tmp2m-56w__ccsm3', 'nmme-prate-34w__ccsm3', 'nmme0-prate-56w__ccsm30', 'nmme0-prate-34w__ccsm30', 'nmme-prate-56w__ccsm3', 'nmme-tmp2m-34w__ccsm3', 'ccsm30']\n",
      "cat vars: ['startdate', 'climateregions__climateregion']\n"
     ]
    }
   ],
   "source": [
    "z = ZipFile('data/widsdatathon2023.zip')\n",
    "train = df_shrink(pd.read_csv(z.open('train_data.csv'), parse_dates=[\"startdate\"])).drop('index', axis=1)\n",
    "test = df_shrink(pd.read_csv(z.open('test_data.csv'), parse_dates=[\"startdate\"])).drop('index', axis=1)\n",
    "submit = df_shrink(pd.read_csv(z.open('sample_solution.csv')))\n",
    "target = 'contest-tmp2m-14d__tmp2m'\n",
    "print(f'{train.isna().any().sum()} cols with null: {train.columns[train.isna().any()].tolist()}')\n",
    "print(f'cat vars: {list(train.select_dtypes(exclude=np.number).columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb45a6",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8fa003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def imputer(df): # multivariate imputer: fill missing values\n",
    "    temp = df.select_dtypes(include=np.number)\n",
    "    df1 = df.copy()\n",
    "    df1[temp.columns] = IterativeImputer(n_nearest_features=10, skip_complete=True, \n",
    "                                         random_state=seed).fit_transform(temp)\n",
    "    return df1\n",
    "\n",
    "def drop_outliers(df, thres=3): # drop rows with outliers\n",
    "    col = df.drop(columns=['lat', 'lon', 'startdate', 'climateregions__climateregion'], \n",
    "                  errors='ignore').columns\n",
    "    return df[(np.abs(stats.zscore(df[col])) < thres).all(axis=1)]\n",
    "\n",
    "def corr(df, thres=0.85): # find highly correlated columns\n",
    "    matrix = df.corr().abs()\n",
    "    matrix = matrix.where(np.triu(np.ones(matrix.shape), k=1).astype(bool))\n",
    "    drop = [c for c in matrix.columns if any(matrix[c] > thres)]\n",
    "    if target in drop: drop.remove(target)\n",
    "    return drop\n",
    "\n",
    "def encode(df): \n",
    "    df['coor']  = df.groupby(['lat','lon']).ngroup()\n",
    "    df['climateregions__climateregion'] = LabelEncoder().fit_transform(df['climateregions__climateregion'])\n",
    "    df = df.drop(columns=['lat', 'lon'])\n",
    "    return df\n",
    "\n",
    "def add_col(df): \n",
    "    df['year']  = df['startdate'].dt.year\n",
    "    df['month'] = df['startdate'].dt.month\n",
    "    df['day']   = df['startdate'].dt.day\n",
    "    df = df.drop(columns=['startdate'])\n",
    "    return df\n",
    "\n",
    "def xy(df): \n",
    "    X = df.drop(columns=target)\n",
    "    y = df[target]\n",
    "    return X, y\n",
    "\n",
    "def split(df, date):\n",
    "    train = df[df.startdate <  date]\n",
    "    test  = df[df.startdate >= date]\n",
    "    X_train, y_train = xy(train)\n",
    "    X_test,  y_test  = xy(test)\n",
    "    return (X_train.reset_index(drop=True), y_train.reset_index(drop=True), \n",
    "            X_test.reset_index(drop=True), y_test.reset_index(drop=True))\n",
    "\n",
    "def feature_engineering(train, test):\n",
    "    train1 = imputer(train)\n",
    "#     print(f'{train1.isna().any().sum()} cols with null')\n",
    "\n",
    "    # train1 = drop_outliers(train1)\n",
    "    # print(f'{len(train) - len(train1)} rows dropped: ')\n",
    "    \n",
    "    # train1 = train1.drop(columns=corr(train1))\n",
    "#     print(f'{len(corr(train1))} / {train.shape[1]} cols dropped')\n",
    "    \n",
    "    df = pd.concat([train1, test[train1.columns.intersection(test.columns)]], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    df = encode(df)\n",
    "    X, y, X_test, _ = split(df, date='2020')\n",
    "    \n",
    "    return X, y, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fda1ab",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3ea4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (343866, 243) | val: (31868, 243) | test: (31354, 243)\n"
     ]
    }
   ],
   "source": [
    "def scale(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_sc = scaler.fit_transform(train)\n",
    "    test_sc  = pd.DataFrame(scaler.transform(test), columns=train.columns)\n",
    "    train_sc = pd.DataFrame(train_sc, columns=train.columns)\n",
    "    return train_sc, test_sc\n",
    "\n",
    "def savePred(pred, index, filename=None, save=False):\n",
    "    df = pd.DataFrame({target:pred, 'index': index})\n",
    "    if save==True:\n",
    "        df.to_csv(f'{filename}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    return mean_squared_error(actual, predicted, squared=False)\n",
    "\n",
    "X, y, X_test = feature_engineering(train.copy(), test.copy())\n",
    "\n",
    "X_train, y_train, X_val, y_val = split(pd.concat([X, y], axis=1), date='2016-7-1')\n",
    "print(f'train: {X_train.shape} | val: {X_val.shape} | test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02a8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2m_34 = train.columns[train.columns.str.contains('nmme0-tmp2m-34w')].tolist()\n",
    "tmp2m_56 = train.columns[train.columns.str.contains('tmp2m-56w')].tolist()\n",
    "featr_14 = train.columns[train.columns.str.contains('14d')].tolist()\n",
    "useful = tmp2m_34 + tmp2m_56 + featr_14\n",
    "if target in useful: useful.remove(target)\n",
    "# [c for c in train.columns.values if item not in useful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b91768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed8c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "y_res = y.copy()\n",
    "nVal = int(len(X_val) / X.coor.nunique()) -1\n",
    "rr = []\n",
    "rr1 = []\n",
    "\n",
    "for j in range(X.coor.nunique()):\n",
    "    x_T = X[X.coor == j][useful]\n",
    "    x_T = pd.DataFrame(scaler.fit_transform(x_T), \n",
    "                       columns=x_T.columns, index=x_T.index)\n",
    "    y_T = y[X.coor == j]\n",
    "    x_t = X_test[X_test.coor == j][useful]\n",
    "    x_t = pd.DataFrame(scaler.transform(x_t), \n",
    "                       columns=x_t.columns, index=x_t.index)\n",
    "    x = pd.concat([x_T, x_t])\n",
    "    \n",
    "    p = -28 ## Lag nmme_tm2m_56w_features 28 days\n",
    "    \n",
    "    ## Creating shifted features \n",
    "    x[tmp2m_34] = x[tmp2m_34].shift(-14).ffill()\n",
    "    x[tmp2m_56] = x[tmp2m_56].shift(p).ffill(limit=20)\n",
    "    x2 = x.iloc[:-nVal,:]\n",
    "    x_test2 = x.iloc[-nVal:p+20,:]\n",
    "    \n",
    "    las = Lasso(alpha=0.019, max_iter=10000)\n",
    "    \n",
    "    ## Prediction without shift\n",
    "    las.fit(x_T, y_T)\n",
    "    train_res= las.predict(x_T)\n",
    "    test_res = las.predict(x_t)\n",
    "    \n",
    "    ## Prediction with shift\n",
    "    las.fit(x2, y_T)\n",
    "#     train_res_1 = las.predict(x2)\n",
    "    test_res_1  = las.predict(x_test2)\n",
    "\n",
    "    df_test   = pd.DataFrame({target:test_res},   index=x_t.index)\n",
    "    df_test_1 = pd.DataFrame({target:test_res_1}, index=x_t.index[:p+20])\n",
    "    \n",
    "    ## calculating the residuals \n",
    "    y_res[X.coor==j] = y_res[X.coor==j] - train_res\n",
    "    \n",
    "    rr.append(df_test)\n",
    "    rr1.append(df_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec231e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.concat(rr)\n",
    "sub1 = pd.concat(rr1)\n",
    "sub.loc[sub.index.isin(sub1.index.values), target] = sub1[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37761c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = corr(X)\n",
    "X1 = drop_outliers(X.drop(columns=drop, errors='ignore'))\n",
    "X1 = add_col(X1)\n",
    "X1_test = X_test.drop(columns=drop, errors='ignore')\n",
    "X1_test = add_col(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9d1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "cbr = CatBoostRegressor(verbose=0, loss_function=\"RMSE\", l2_leaf_reg=2.3, \n",
    "                        max_depth=6, iterations=25000)\n",
    "cbr.fit(X1, y_res.loc[X1.index], cat_features=['coor'])\n",
    "y_pred_cat = cbr.predict(X1_test)\n",
    "\n",
    "\n",
    "lgb = LGBMRegressor(objective='regression', metric='rmse', \n",
    "                    categorical_feature=X1.columns.get_loc('coor'))\n",
    "lgb.fit(X1, y_res.loc[X1.index], verbose=0)\n",
    "y_pred_lgb = lgb.predict(X1_test)\n",
    "\n",
    "\n",
    "xgb = XGBRegressor(objective=\"reg:squarederror\", eval_metric='rmse', \n",
    "                   random_state=seed)\n",
    "xgb.fit(X1, y_res.loc[X1.index], verbose=0)\n",
    "y_pred_xgb = xgb.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "119a082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest-tmp2m-14d__tmp2m</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.246001</td>\n",
       "      <td>375734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.155734</td>\n",
       "      <td>375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.136013</td>\n",
       "      <td>375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.061622</td>\n",
       "      <td>375737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.067243</td>\n",
       "      <td>375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>7.101493</td>\n",
       "      <td>407083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>7.253449</td>\n",
       "      <td>407084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>6.636917</td>\n",
       "      <td>407085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>6.785509</td>\n",
       "      <td>407086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>7.357269</td>\n",
       "      <td>407087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contest-tmp2m-14d__tmp2m   index\n",
       "0                     30.246001  375734\n",
       "1                     30.155734  375735\n",
       "2                     30.136013  375736\n",
       "3                     30.061622  375737\n",
       "4                     30.067243  375738\n",
       "...                         ...     ...\n",
       "31349                  7.101493  407083\n",
       "31350                  7.253449  407084\n",
       "31351                  6.636917  407085\n",
       "31352                  6.785509  407086\n",
       "31353                  7.357269  407087\n",
       "\n",
       "[31354 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred_lgb*0.10+y_pred_cat*0.60+y_pred_xgb*0.30\n",
    "\n",
    "sub[\"contest-tmp2m-14d__tmp2m\"] = sub[\"contest-tmp2m-14d__tmp2m\"] + y_pred\n",
    "# sub = sub[[\"contest-tmp2m-14d__tmp2m\", \"index\"]]\n",
    "sub['index'] = submit['index']\n",
    "sub.to_csv('submission.csv', index = False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# params = {'iterations': 15000,\n",
    "#           'learning_rate': 0.01,\n",
    "#           'depth': 6,\n",
    "#           'l2_leaf_reg': 3,\n",
    "#           'bagging_temperature': 0,\n",
    "#           'border_count': 128,\n",
    "#           'loss_function': 'RMSE',\n",
    "#           'random_seed': seed,\n",
    "#           'verbose': False}\n",
    "\n",
    "# model = CatBoostRegressor(**params)\n",
    "# model.fit(X_train, y_train,  eval_set=(X_val, y_val))\n",
    "# savePred(model.predict(X_test), testIndex, filename='s2', save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a642351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
